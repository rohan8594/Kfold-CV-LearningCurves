## 1. Overview

In this project we apply Naïve Bayesian and Decision Tree classifiers to a Census Income
dataset and then use popular evaluation techniques like k-fold cross validation and learning
curves to evaluate our machine learning model and diagnose whether these two classifiers
suffer from any underfitting or overfitting symptoms. The main goal of this project is to
follow a typical machine learning approach to solving any classification problem by first
understanding our data through analysis and visualization, then applying the classification
technique and then finally using a sound evaluation strategy to further improve our results.

The dataset used for this project is the Census Income or Adult dataset, which is available at
[http://archive.ics.uci.edu/ml/datasets/Adult.](http://archive.ics.uci.edu/ml/datasets/Adult.) The dataset contains 48842 instances
(train=32561, test=16281) that contain a mix of both continuous and discrete features. The
features themselves are various demographic characteristics such as age, education, marital-
status, occupation, race, sex, etc., and the class attribute is the income status (i.e. whether
income is >50K or <=50K).

## 2. Methodology / Main Steps

The following section gives a brief description of the methodology and main steps that have
been adopted to accomplish this task.

### 2.1 Know Your Data

The first step is to perform exploratory data analysis on our dataset with the aim of analysing
and understanding the data better. We use Tableau to visualize and explore the relationship
between each attribute and the class label, aiming to gain an initial understanding of the
dataset. Also, the relationships between each pair of continuous attributes are visualized to
see whether the dataset contains highly correlated attributes. Observations and insights from
this step are covered in the final ‘Evaluation and Analysis’ section of this document.

### 2.2 Classification

Once we have a good understanding of the data, we apply Weka’s implementation of the
Naïve Bayes and Decision Tree classifiers (weka.classifiers.bayes.NaiveBayes and
weka.classifiers.trees.J48 respectively) to perform the classification task. To do this, I use
Python’s wrapper for Weka (https://github.com/fracpete/python-weka-wrapper3) to call
Weka’s APIs to these two classifiers.

In this step, I load the training data from the ‘adult.csv’ file, build the two classifiers on the
entire training set, and then use the classifiers to label the income status of each instance.
Then I count the number of instances that have been classified into each class to simply get
an initial idea of how the classifiers are performing.


### 2.3 K-fold Cross Validation

The next step is to implement a k-fold cross validation strategy to evaluate the performance
of the Naïve Bayes and Decision Tree Classifiers on the Adult dataset with k=5 and 10,
respectively. We first randomize the data, then we generate k different training folds and
validation folds respectively and build our classifier on each training fold and test it on its
subsequent validation fold. We then check the average error rate generated in the cross-
validation to get an estimate of how well our classifier would predict in real practice.

### 2.4 Learning Curve

The next evaluation strategy we apply is the learning curve. Learning curves are used to
diagnose bias and variance and thereby diagnose whether our classifier shows any
underfitting or overfitting symptoms. The learning curve is generated by plotting the error vs.
the training set size (i.e., how better does the model get at predicting the target as you the
increase number of instances used to train it). It is important to note that in the learning curve,
there are two error scores that are monitored: one for the validation set, and one for the
training sets. We plot the evolution of the two error scores as training sets change and end up
with two curves. The evolution of the two curves over the training set size gives us a good
indication of bias and variance in our model.

### 2.5 Applying Classifier to Test Data

In the final step we simply apply the two learned classifiers (Naïve Bayes and Decision Tree)
to the ‘adult_test.csv’ and report their precision, recall, F1-measure, and accuracy.

## 3. Instructions for Compiling

The project folder contains a total of four python scripts namely, classification.py,
kfold_crossvalidation.py, learning_curve.py, and test.py that must be compiled to produce the
final results. There are also two csv files, adult.csv and adult_test.csv that contain the training
and test data respectively, and a Tableau workbook titled Book1.twb that contains
visualization performed for ‘2.1 - Know Your Data’ step. The step-wise instructions to
compile and run the program are given below.
<br/>
a) For this project, I have used python-weka-wrapper3 that allows you to use Weka from
within Python3. To compile and run all the scripts in this project you will first need to install
python-weka-wrapper3 on your system. The library has the following requirements:

* Python3 (does not work with Python 2)
* javabridge (>=1.0.14)
* Oracle JDK 1.8+

I have used the linux bash shell within windows for this project. If you are using Ubuntu or
the linux bash shell in windows to execute this project, please follow the installation
instructions given below to install python-weka-wrapper3, javabridge and Oracle JDK.


First, you need to be able to compile C/C++ code and Python modules. Use the following
command:
```
$ sudo apt-get install build-essential python3-dev
```
Now, you can install the various packages that we require for installing python-weka-
wrapper3:
```
$ sudo apt-get install python3-pip python3-numpy
```
Install OpenJDK as well, in order to get all the header files that javabridge compiles against
(but don’t use it for starting up JVMs):
```
$ sudo apt-get install default-jdk
```
Finally, you can use pip3 to install the Python packages that are not available in the
repositories:
```
$ sudo pip3 install javabridge

$ sudo pip3 install python-weka-wrapper
```
NOTE: If you are using Mac or any other OS for this project, please follow the
installation instructions on [http://fracpete.github.io/python-weka-wrapper3/install.html](http://fracpete.github.io/python-weka-wrapper3/install.html)

**Please also note, if you’re using Mac** , **you might get an error “Python was unable to find
JVM”. In this case, please execute the following command in your terminal:**
```
**$ export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home**
```
<br/>
b) Once you have installed python-weka-wrapper3, you can open your terminal and navigate
to the project directory. The first script to execute is classification.py to perform the initial
classification task. Use the following command:

```
$ python3 classification.py
```

<br/>
c) Next you can execute the k-fold cross validation module which can be found in the
kfold_crossvalidation.py script:

```
$ python3 kfold_crossvalidation.py k
```

where k = 5 or 10

<br/>
d) Next you can execute the learning curve module which is present in the learning_curve.py
script:

```
$ python3 learning_curve.py k
```

where k = 5 or 10


**NOTE: In order to plot the learning curve, you will need python’s matplotlib library
installed. To install this, execute the command:**
```
**$ pip install matplotlib**
```
<br/>
e) The final step is to apply the two learned classifiers to the test dataset found in
‘adult_test.csv’. Use the following command:

```
$ python test.py
```
